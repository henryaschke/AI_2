{"cells":[{"cell_type":"markdown","id":"817705cd","metadata":{"id":"817705cd"},"source":["# Assignment 2 – Question&nbsp;1 Notebook  \n","**Tile‑Coded Approximate Q‑Learning for Drone Navigation**\n","\n","This notebook maps **each sub‑question or sub-part** of Question&nbsp;1 to specific cells:\n","\n","| Sub‑question | Notebook section |\n","|--------------|------------------|\n","| (a) Feature representation & update equation | See markdown Section A and the Tile Coder + `update` method |\n","| (b) Implementation (environment + agent) | Section B code cells |\n","| (c) Hyper‑parameter tuning | Section C code cell |\n","| (d) Performance evaluation | Section D code cell |\n"]},{"cell_type":"markdown","id":"22455cbb","metadata":{"id":"22455cbb"},"source":["## **A. Sub‑question (a): Feature Representation & Update Equation**\n","\n","We discretise the continuous position $(x,y)$ using a uniform grid  \n","implemented by **`TileCoder`**.  \n","The linear value function for a given action \\(a\\) is  \n","\n","$\n","\\hat Q(s,a) = \\mathbf w_a^\\top \\; \\boldsymbol\\phi(s)\n","$\n","\n","where $\\boldsymbol\\phi(s)$ is a one‑hot vector with `1` at the active tile.\n","\n","**TD update used in `ApproxQLearner.update`:**\n","\n","$$\n","\\delta =\n","\\begin{cases}\n","r - Q(s,a) & \\text{if } s' \\text{ is terminal},\\\\[4pt]\n","r + \\gamma \\max_{a'} Q(s',a') \\, - \\, Q(s,a) & \\text{otherwise}.\n","\\end{cases}\n","\\qquad\n","\\mathbf w_a \\leftarrow \\mathbf w_a + \\alpha\\,\\delta\\,\\boldsymbol\\phi(s)\n","$$\n","\n","Below is the code for the tile coder and comments highlighting\n","where this update is implemented.\n"]},{"cell_type":"code","execution_count":null,"id":"76568931","metadata":{"id":"76568931"},"outputs":[],"source":["\n","from typing import Tuple\n","from dataclasses import dataclass\n","import numpy as np\n","\n","# -------- Tile Coder (Sub‑Q1 a) ----------------------------------------- #\n","@dataclass\n","class TileCoder:\n","    \"\"\"Uniform grid over (x,y) – wind dimension ignored for features.\"\"\"\n","    nx: int\n","    ny: int\n","    x_range: Tuple[float, float] = (0.0, 1.0)\n","    y_range: Tuple[float, float] = (0.0, 1.0)\n","\n","    def n_tiles(self) -> int:\n","        return self.nx * self.ny\n","\n","    def tile_index(self, state: np.ndarray) -> int:\n","        \"\"\"Map (x,y) to a single tile id.\"\"\"\n","        x, y = state[0], state[1]\n","\n","        # Compute discrete bin indices\n","        ix = ### fill up here\n","        iy = ### fill up here\n","\n","        # Row‑major flatten\n","        return iy * self.nx + ix\n"]},{"cell_type":"markdown","id":"63d06134","metadata":{"id":"63d06134"},"source":["## **B. Sub‑question (b): Implementation (Environment & Agent)**\n","\n","The next two code cells provide:\n","\n","* `DroneWindEnv` — the windy‑field environment  \n","* `ApproxQLearner` — the agent implementing the TD update from Section A  \n","  *Key lines are commented as “Sub‑Q1 b”.*\n"]},{"cell_type":"code","execution_count":null,"id":"968da950","metadata":{"id":"968da950"},"outputs":[],"source":["\n","import numpy as np\n","\n","# -------- Environment (Sub‑Q1 b) --------------------------------------- #\n","class DroneWindEnv:\n","    \"\"\"Continuous 2‑D field with global east–west wind.\"\"\"\n","\n","    def __init__(self, max_steps: int = 200, seed: int | None = None):\n","        self.rng = np.random.default_rng(seed)\n","        self.max_steps = max_steps\n","        self.action_space = 4  # 0=N,1=S,2=E,3=W\n","        self.state: np.ndarray | None = None\n","        self._step_count = 0\n","\n","    # -- physics\n","    @staticmethod\n","    def _next_state(state: np.ndarray, action: int) -> np.ndarray:\n","        x, y, w = state\n","        base_move = 0.05\n","\n","        if action == 0:   # North\n","            dx, dy = 0.0, base_move\n","        elif action == 1: # South\n","            dx, dy = 0.0, -base_move\n","        elif action == 2: # East (head‑wind)\n","            dx, dy = base_move * (1.0 - 0.5 * w), 0.0\n","        elif action == 3: # West (tail‑wind)\n","            dx, dy = -base_move * (1.0 + 0.5 * w), 0.0\n","        else:\n","            raise ValueError(\"Bad action\")\n","\n","        new_x = np.clip(x + dx, 0.0, 1.0)\n","        new_y = np.clip(y + dy, 0.0, 1.0)\n","        new_w = np.clip(w + 0.01 * (np.random.rand() - 0.5), 0.0, 1.0)\n","\n","        return np.array([new_x, new_y, new_w], dtype=np.float32)\n","\n","    # -- Gym‑like API\n","    def reset(self) -> np.ndarray:\n","        self.state = np.array([0.05, 0.05, self.rng.random()], dtype=np.float32)\n","        self._step_count = 0\n","        return self.state.copy()\n","\n","    def step(self, action: int):\n","        self._step_count += 1\n","        next_state = self._next_state(self.state, action)\n","\n","        # +10 in charging zone (NE corner), else ‑1 per step\n","        reward = 10.0 if (next_state[0] > 0.9 and next_state[1] > 0.9) else -1.0\n","        done = reward == 10.0 or self._step_count >= self.max_steps\n","\n","        self.state = next_state\n","        return next_state.copy(), reward, done, {}\n"]},{"cell_type":"code","execution_count":null,"id":"a88389ce","metadata":{"id":"a88389ce"},"outputs":[],"source":["\n","# -------- Agent (Sub‑Q1 b) --------------------------------------------- #\n","class ApproxQLearner:\n","    \"\"\"Linear Approximate Q‑learning with a single tiling.\"\"\"\n","\n","    def __init__(self,\n","                 tile_coder: TileCoder,\n","                 n_actions: int,\n","                 alpha: float = 0.1,\n","                 gamma: float = 0.99,\n","                 epsilon: float = 0.1,\n","                 seed: int | None = None):\n","        self.tc = tile_coder\n","        self.n_actions = n_actions\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.rng = np.random.default_rng(seed)\n","\n","        # Weight matrix w[a, tile]\n","        self.weights = np.zeros((n_actions, self.tc.n_tiles()), dtype=np.float32)\n","\n","    # -- helpers\n","    def _phi(self, state: np.ndarray) -> int:\n","        return self.tc.tile_index(state)\n","\n","    def q_values(self, state: np.ndarray) -> np.ndarray:\n","        tid = self._phi(state)\n","        return self.weights[:, tid]\n","\n","    def select_action(self, state: np.ndarray) -> int:\n","        if self.rng.random() < self.epsilon:\n","            return self.rng.integers(self.n_actions)\n","        return int(np.argmax(self.q_values(state)))\n","\n","    # -- TD update implementing Sub‑question (a) formula\n","    def update(self, s: np.ndarray, a: int, r: float, s_next: np.ndarray, done: bool):\n","        tid = self._phi(s)\n","        q_sa = self.weights[a, tid]                # current estimate\n","\n","        target = ### fill up here\n","        td_error = ### fill up here                  # δ\n","        self.weights[a, tid] += ### fill up here  # gradient step\n","\n","    # -- training loop\n","    def train(self, env: DroneWindEnv, episodes: int = 1000, max_steps: int = 200):\n","        history = []\n","        ### fill up here\n","        return history\n","\n","    # -- evaluation\n","    def evaluate(self, env: DroneWindEnv, episodes: int = 100, max_steps: int = 200):\n","        total = 0.0\n","        ### fill up here\n","        return total / episodes\n"]},{"cell_type":"markdown","id":"1604e667","metadata":{"id":"1604e667"},"source":["## **C. Sub‑question (c): Hyper‑parameter Tuning**\n","\n","Adjust the parameters below and re‑run training to observe their impact.\n","Document your chosen values in the assignment write‑up (pdf file).\n"]},{"cell_type":"code","execution_count":null,"id":"f07d608b","metadata":{"id":"f07d608b"},"outputs":[],"source":["\n","# --- Hyper‑parameters (Sub‑Q1 c) --------------------------------------- #\n","NX, NY = 10, 10        # tile resolution\n","ALPHA   = 0.2          # learning rate\n","GAMMA   = 0.95         # discount factor\n","EPSILON = 0.2          # initial exploration rate\n","SEED    = 42\n","\n","env   = DroneWindEnv(max_steps=200, seed=SEED)\n","tc    = TileCoder(nx=NX, ny=NY)\n","agent = ApproxQLearner(tc, n_actions=env.action_space,\n","                       alpha=ALPHA, gamma=GAMMA,\n","                       epsilon=EPSILON, seed=SEED)\n","\n","print(\"Training...\")\n","agent.train(env, episodes=500)\n"]},{"cell_type":"markdown","id":"142780af","metadata":{"id":"142780af"},"source":["## **D. Sub‑question (d): Performance Evaluation**\n","\n","Run the cell below to compute the average reward over 100 evaluation\n","episodes and fill in the rubric table in your PDF solution.\n"]},{"cell_type":"code","execution_count":null,"id":"3aa8c43e","metadata":{"id":"3aa8c43e"},"outputs":[],"source":["\n","avg_reward = agent.evaluate(env, episodes=100)\n","print(f\"Average reward over 100 evaluation episodes: {avg_reward:.2f}\")\n"]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}